{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e628e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "import shap\n",
    "import joblib\n",
    "import requests\n",
    "from explainerdashboard import ClassifierExplainer, ExplainerDashboard\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25802bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set general font size\n",
    "plt.rcParams['font.size'] = '13'\n",
    "\n",
    "# dpi\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89510303",
   "metadata": {},
   "source": [
    "## version 1) \n",
    "\n",
    "daily version: keeping latest datapoint on a day for each pool, inference will run hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82988401",
   "metadata": {},
   "source": [
    "### dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88354d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/pools_2022_05_20.csv')\n",
    "dfEnriched = pd.read_json('../data/dataEnriched_2022_05_20.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dba297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape, dfEnriched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4c6a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merging poolinfo columns to df\n",
    "poolInfoCols = [\n",
    "    'chain',\n",
    "    'project',\n",
    "    'pool',\n",
    "    'stablecoin', \n",
    "    'ilRisk',\n",
    "    'exposure',\n",
    "]\n",
    "\n",
    "df = df.merge(dfEnriched[poolInfoCols], how='left', on=['pool', 'chain', 'project'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14053f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93c447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.apy.quantile(.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ee9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step1) remove rows with extreme apy values\n",
    "df = df[(df.apy >= 0) & (df.apy <= df.apy.quantile(.999))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc566e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666fa13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step2) keep only the latest value on a given day per pool\n",
    "# reason: i assume the daily fluctuation is stable so less interesting. also this will reduce the dataset by\n",
    "# a lot but will speed up prototyping. could come back to an hourly version but for now want to keep it simple\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "# sort before group\n",
    "df = df.sort_values(['pool', 'timestamp']).reset_index(drop=True)\n",
    "df = df.groupby(['pool', pd.Grouper(key='timestamp', freq='1D')]).last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1357f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8f234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for the target decided to use median instead of avg cause its robust against any remaining outliers\n",
    "# eg imagine a pool which starts at 20% and declined to 2% within 4weeks, but has a single outlier on a particular \n",
    "# day where apy might be 200% -> using an average for that would be crap cause the target will be skewed by that\n",
    "# single outlier. hence median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281b89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(['pool', 'timestamp']).reset_index(drop=True)\n",
    "# we want to predict if a pool can keep its apy for the next 30days\n",
    "horizon = 30\n",
    "df = df.assign(\n",
    "    apyMedianShifted=df.groupby('pool')['apy'].apply(lambda x: x.rolling(horizon).median().shift(-(horizon-1)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f985d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step3) add some basic backward looking stats as features:\n",
    "df = df.sort_values(['pool', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "grouping = df.groupby('pool')\n",
    "df['apyMeanExpanding'] = grouping['apy'].apply(lambda x: x.expanding().mean())\n",
    "df['apyStdExpanding'] = grouping['apy'].apply(lambda x: x.expanding().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06bade3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.apyStdExpanding = df.apyStdExpanding.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fedb3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6b203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we keep a copy which we use later on as reference data (the last day in the dataset)\n",
    "df_full = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086be2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full.timestamp.min(), df_full.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d933ba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full = df_full.sort_values(['pool', 'timestamp']).reset_index(drop=True)\n",
    "df_reference = df_full.groupby('pool').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a2d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04bc0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['apyMedianShifted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5be7ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a96b6",
   "metadata": {},
   "source": [
    "calculate the ML target variable\n",
    "\n",
    "```\n",
    "y = {\n",
    "    0: if ((apyFuture - apy) / apy) < -0.2\n",
    "    1: else\n",
    "}\n",
    "```\n",
    "\n",
    "so if the pct-change btw forward looking 30day median apy to todays apy is < -20%, we assign label 0 (== pool apy unstable), in all other cases, we consider it stable.\n",
    "threshold of -20% is just something i thought I'd still consider to be kinda stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f9cc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['pct'] = (df['apyMedianShifted'] - df['apy']) / df['apy']\n",
    "# note: in case of apy == 0 -> pct will be NaN and target will be encoded as 1 (wich is what we want cause the \n",
    "# apyMedianShifted from 30days forward looking is greater than >= 0)\n",
    "df['target'] = np.where(df['pct'] < -0.2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a4f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target distribution\n",
    "df['target'].value_counts().plot.bar()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aebbb1",
   "metadata": {},
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factorize cat variables\n",
    "cols_to_factorize = ['chain', 'project', 'ilRisk', 'exposure', 'stablecoin']\n",
    "\n",
    "for i in cols_to_factorize:\n",
    "    df[f'{i}_factorized'] = pd.factorize(df[i])[0]\n",
    "\n",
    "# save mapping (which we use on the triggerEnrichment lambda)\n",
    "project = df[['project', 'project_factorized']].set_index('project')\n",
    "chain = df[['chain', 'chain_factorized']]\n",
    "\n",
    "mapping_project = df.set_index('project')[['project_factorized']].to_dict()\n",
    "mapping_chain = df.set_index('chain')[['chain_factorized']].to_dict()\n",
    "\n",
    "d_cat_map = {}\n",
    "\n",
    "d_cat_map.update(mapping_project)\n",
    "d_cat_map.update(mapping_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09d3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# leaving out all weak features for now\n",
    "features = [\n",
    "    'apy',\n",
    "    'tvlUsd',\n",
    "    'apyMeanExpanding',\n",
    "    'apyStdExpanding',\n",
    "    'chain_factorized',\n",
    "    'project_factorized',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d7c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_state = 1993"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f1283",
   "metadata": {},
   "source": [
    "Pale Rider#9068 provided some useful feedback to make setup more robust and less biassed during performance evaluation. Instead of simply assuming iid (which is a very strong assumption given the data clearly has some\n",
    "temporal structure I will move away from just randomly splitting into train and test towards\n",
    "slicing off a test set based on timestamp so that the training data doesn't include any future data)\n",
    "\n",
    "Also, I think the cross validation process can further be improved by using time series split instead of vanilla\n",
    "cv. The below is a minimal effort implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5cf40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first sort on timestamp anscending\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c0a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.timestamp.min(), df.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee780e50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find an optimal cutoff date so that test set consists of roughly 30% of dataset\n",
    "g = df.groupby(pd.Grouper(key='timestamp', freq='D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a367b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find split point ('taking last 30% as test set')\n",
    "z = g.size().cumsum() / g.size().sum()\n",
    "\n",
    "# find index closest to 0.7\n",
    "train_size_pct = 0.7\n",
    "idx = np.abs(z.values - train_size_pct).argmin()\n",
    "\n",
    "cutoff_date = z.index[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19581737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cutoff_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9140e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = df[df.timestamp <= cutoff_date]\n",
    "X_test = df[df.timestamp > cutoff_date]\n",
    "\n",
    "y_train = X_train['target']\n",
    "y_test = X_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b02e73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77190ef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.timestamp.min(), X_train.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9f1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.timestamp.min(), X_test.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ec2ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train[features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9382e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9322f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using 2 algos with out of box settings, logreg cause basic and random forest cause usually better\n",
    "# and checking for increase in cv -> increase in test? (as a consistency\n",
    "# check if test set is similarly distributed to train)\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "clf_rf = RandomForestClassifier(random_state=random_state, n_estimators=100, n_jobs=-1, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6674c47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_state = 1993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f6b95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc40c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorting data again with second column being pool, so that the sort is deterministic and we get same results\n",
    "# on repeated runs (sort on timestamp alone might still result in different sort order as not unique)\n",
    "X_train = X_train.sort_values([\"timestamp\", 'pool'], ascending=True).reset_index(drop=True)\n",
    "y_train = X_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3131fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d209824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74b273",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = defaultdict(list)\n",
    "\n",
    "# running time series split\n",
    "for algo, algoname in zip([clf_lr, clf_rf], ['logreg', 'randomforest']):\n",
    "    print(f'Running ts split for {algo}')\n",
    "    for i, (train_idx, val_idx) in enumerate(ts.split(X_train.values)):\n",
    "\n",
    "        print(f\"train on fold 0-{i}, validate on fold {i+1}\")\n",
    "        print(f\"train_idx: {train_idx}, test_idx: {val_idx}\")\n",
    "\n",
    "        X_train_ts, X_val_ts = X_train.loc[train_idx], X_train.loc[val_idx]\n",
    "        y_train_ts, y_val_ts = X_train_ts['target'], X_val_ts['target']\n",
    "\n",
    "        algo.fit(X_train_ts[features].values, y_train_ts.values)\n",
    "        y_pred_val = algo.predict_proba(X_val_ts[features].values)[:, 1]\n",
    "        roc_score = roc_auc_score(y_val_ts.values, y_pred_val)\n",
    "        print(f\"roc-auc {roc_score:.5f}\\n\")\n",
    "        d[algoname].append(roc_score)\n",
    "        \n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4220ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(d['logreg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d68f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(d['randomforest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d8b3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check what features drive performance via feature importance plot\n",
    "clf_rf.fit(X_train[features].values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f969f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check oob score\n",
    "clf_rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9182c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importances = clf_rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf_rf.estimators_], axis=0)\n",
    "forest_importances = pd.Series(importances, index=features).sort_values(ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "forest_importances.plot.barh(yerr=std, ax=ax)\n",
    "ax.set_title(\"feature importances\")\n",
    "ax.set_ylabel(\"mean decrease in impurity\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e108848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test performance\n",
    "y_pred_test = clf_rf.predict_proba(X_test[features].values)[:, 1]\n",
    "roc_auc_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3c1f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "RocCurveDisplay.from_estimator(clf_rf, X_test[features].values, y_test, ax=ax)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153193c",
   "metadata": {},
   "source": [
    "#### error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9c369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check performance on a project level (want to see if there is a project for which predictions are\n",
    "# significantly worse than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062357f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train['y_pred'] = clf_rf.predict_proba(X_train[features].values)[:, 1]\n",
    "X_train['abs_delta'] = abs(X_train['y_pred'] - X_train['target'])\n",
    "\n",
    "X_test['y_pred'] = clf_rf.predict_proba(X_test[features].values)[:, 1]\n",
    "X_test['abs_delta'] = abs(X_test['y_pred'] - X_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9f355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# on project level\n",
    "X_train.groupby('project')['abs_delta'].describe().sort_values('75%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa208d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# on chain level\n",
    "X_train.groupby('chain')['abs_delta'].describe().sort_values('75%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c51b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8), ncols=2, nrows=2)\n",
    "X_train['y_pred'].plot.hist(ax=ax[0, 0], title='y-pred distribution [train]')\n",
    "X_train['abs_delta'].plot.hist(ax=ax[0, 1], title='abs-delta distribution [train]')\n",
    "\n",
    "\n",
    "X_test['y_pred'].plot.hist(ax=ax[1, 0], title='y-pred distribution [test]')\n",
    "X_test['abs_delta'].plot.hist(ax=ax[1, 1], title='abs-delta distribution [test]')\n",
    "\n",
    "ax[0, 0].grid(True)\n",
    "ax[0, 1].grid(True)\n",
    "ax[1, 0].grid(True)\n",
    "ax[1, 1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d8704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check individual examples of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d69cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_sample(pool: str):\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    df_full[df_full['pool'] == pool].set_index('timestamp')['apy'].plot(ax=ax)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe701659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'pool', \n",
    "    'timestamp', \n",
    "    'chain', \n",
    "    'project', \n",
    "    'apy', \n",
    "    'apyMedianShifted', \n",
    "    'pct', \n",
    "    'target', \n",
    "    'y_pred', \n",
    "    'abs_delta',\n",
    "]\n",
    "X_train[X_train['abs_delta'] == 0][cols].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63198e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_sample('0x53a901d48795C58f485cBB38df08FA96a24669D5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61f027",
   "metadata": {},
   "source": [
    "### shap interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4177db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shap library is quite slow on large number of trees in the ensemble, i train a second instance of rf\n",
    "# on a lower nb of estimators to speed up the shapley part\n",
    "clf_rf_shap = RandomForestClassifier(random_state=random_state, n_estimators=10)\n",
    "clf_rf_shap.fit(X_train[features].values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4adb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf_rf_shap)\n",
    "shap_values = explainer.shap_values(X_train[features].values, check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17165f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], X_train[features], plot_type='dot', plot_size=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0999fef",
   "metadata": {},
   "source": [
    "### interpretation (numerical features only):\n",
    "\n",
    "\n",
    "+ apy: large values of apy (red) lead to a decrease in probability of a pool to maintain the apy over the next 4weeks. this makes sense as larger apy values are often much more volatile\n",
    "\n",
    "+ apyMeanExpanding: larger values of apyMeanExpanding (red) increase the probability of a pool to maintain the apy over the next 4weeks.\n",
    "    again makes sense because if the backward looking mean apy is large then the apy is likely going to be similarly high and stable for the next 4weeks.\n",
    "    \n",
    "+ apyStdExpanding: less clear but i'd interpret it this way: the larger the feature value (red) the lower the probability of a pool to maintain the apy over the next 4weeks. more volatility in the past -> less likely of stable apy in the future\n",
    "\n",
    "+ tvlUsd: bit harder to tell but would say: larger values -> higher probability in prediction which i think is driven by the larger tvl pools with lower apys, but which are more stable overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64485981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### some sanity checks how consistent the predictions will be, eg are they fluctuating a lot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c17656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d612d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full[df_full['project'] == 'curve'].sort_values('tvlUsd', ascending=False).iloc[0, :].pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4621f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = '0xDC24316b9AE028F1497c275EB9192a3Ea0f67022-ethereum'\n",
    "a = X_train[X_train['pool'] == p].sort_values('timestamp')[['timestamp', 'target', 'y_pred']]\n",
    "b = X_test[X_test['pool'] == p].sort_values('timestamp')[['timestamp', 'target', 'y_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb9b198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_sample(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e9b51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.concat([a, b]).sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f440ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# depending on pool, less or more fluctuation btw predictions, eg could be that today prediction is very high for\n",
    "# stable within the next 4weeks, and tomorrow the reverse (if apy is very high tmr for example)\n",
    "# this will get better as we get more training data. assume within the next 4-6weeks i'd retrain everything with \n",
    "# better backward looking statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8bf39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### explainer dashboard has some cool additional insights:\n",
    "# checking on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920ca65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.timestamp.min(), X_test.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca432fe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explainer = ClassifierExplainer(clf_rf_shap, X_test[features], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c9fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db = ExplainerDashboard(explainer, shap_interaction=False, mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3a492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db.run(mode='inline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bda7e6",
   "metadata": {},
   "source": [
    "### train on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766abf9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4e9c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrain on full historical data\n",
    "clf_rf.fit(df[features].values, df['target'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569bf4da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed124e1",
   "metadata": {},
   "source": [
    "### save model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f016d5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save full prepared training data\n",
    "df[features + ['target']].to_csv(\"df_2022_05_20.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240f11b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "joblib.dump(clf_rf, '../artefacts/clf_2022_05_20.joblib', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a46bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save feature list\n",
    "joblib.dump(features, '../artefacts/feature_list.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b2d08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature mappings (required in triggerEnrichment)\n",
    "with open('../artefacts/categorical_feature_mapping_2022_05_20.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(d_cat_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752f2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for monitoring system\n",
    "df_reference['chain_factorized'] = df_reference['chain'].map(d_cat_map['chain_factorized'])\n",
    "df_reference['project_factorized'] = df_reference['project'].map(d_cat_map['project_factorized'])\n",
    "df_reference[features].to_json('../artefacts/reference_data_2022_05_20.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
